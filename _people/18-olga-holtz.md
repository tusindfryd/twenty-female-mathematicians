---
name: Olga Holtz
research_areas: numerical analysis, numerical linear algebra, approximation theory
layout: people

---

<p>Holtz is an applied mathematician working in numerical analysis, numerical linear algebra and approximation theory. As the name suggests, numerical analysis is the subject which studies numerical solutions to problems of analysis. The aim is to find accurate approximate solutions to problems which cannot be solved otherwise. We have already mentioned the Newton-Raphson method as an example. Another important branch of numerical analysis is numerical integration, where one seeks to use a numerical method to evaluate a definite integral which cannot be computed in terms of elementary functions (most integrals fall into this category). Start with an integral</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7Ba%7D%5E%7Bb%7D%7Bf%5Cleft%28%20x%20%5Cright%29%5Ctext%7B%5C%20dx%7D%7D."><br></p></center>
<p>Assume that the integrand can be evaluated at equally spaced points and that the integral can be approximated with a formula of the form</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7Ba%7D%5E%7Bb%7D%7Bf%5Cleft%28%20x%20%5Cright%29%5Ctext%7B%5C%20dx%7D%7D%20%5Capprox%20%5Cfrac%7Bb%20-%20a%7D%7BN%7D%5Csum_%7Bk%20%3D%201%7D%5E%7BN%7D%7Bc_%7Bk%7Df%5Cleft%28%20a%20%2B%20kh%20%5Cright%29%7D."><br></p></center>
<p>A formula of this type is known as a Newton-Cotes formula. For example, set <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?N%20%3D%202">.</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7Ba%7D%5E%7Bb%7D%7Bf%5Cleft%28%20a%20%2B%20y%20%5Cright%29%5Ctext%7B%5C%20dx%7D%7D%20%5Capprox%20%5Cfrac%7Bb%20-%20a%7D%7BN%7D%5Csum_%7Bk%20%3D%201%7D%5E%7BN%7D%7Bc_%7Bk%7D%5Cleft%28%20f%5Cleft%28%20a%20%2B%20kh%20%5Cright%29%20%5Cright%29%7D."><br></p></center>
<p>Use Taylor expansions.</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%5Cleft%28%20a%20%2B%20kh%20%5Cright%29%20%3D%20f%5Cleft%28%20a%20%5Cright%29%20%2B%20khf%5E%7B%27%7D%5Cleft%28%20a%20%5Cright%29%20%2B%20%5Cfrac%7B1%7D%7B2%7Dk%5E%7B2%7Dh%5E%7B2%7Df%5E%7B%27%27%7D%5Cleft%28%20a%20%5Cright%29%20%2B%20O%5Cleft%28%20h%5E%7B3%7D%20%5Cright%29%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%5Cleft%28%20a%20%2B%20y%20%5Cright%29%20%3D%20f%5Cleft%28%20a%20%5Cright%29%20%2B%20%5Cfrac%7B1%7D%7B2%7Dhf%5E%7B%27%7D%5Cleft%28%20a%20%5Cright%29%20-%20%5Cfrac%7B1%7D%7B2%7Dh%5E%7B2%7Df%5E%7B%27%27%7D%5Cleft%28%20a%20%5Cright%29%20%2B%20O%5Cleft%28%20h%5E%7B3%7D%20%5Cright%29%2C"><br></p></center>
<p>where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?h"> is the step size <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cleft%28%20b%20-%20a%20%5Cright%29%2FN">. This gives us a Taylor representation which must be equivalent to the original function being summed:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%5Cleft%28%20a%20%5Cright%29%20%2B%20h%5Cleft%28%20%5Cfrac%7B3%7D%7B2%7D%5Cleft%28%20f%5E%7B%27%7D%5Cleft%28%20a%20%5Cright%29%20%2B%20hf%5E%7B%27%27%7D%5Cleft%28%20a%20%5Cright%29%20%5Cright%29%20%5Cright%29%20%2B%20O%5Cleft%28%20h%5E%7B3%7D%20%5Cright%29."><br></p></center>
<p>By comparison of coefficients, we have <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?c_%7B1%7D%20%3D%20c_%7B2%7D%20%3D%203%2F2">. This gives us a Newton-Cotes formula known as the trapezoid rule:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7Ba%7D%5E%7Bb%7D%7Bf%5Cleft%28%20x%20%5Cright%29%5Ctext%7B%5C%20dx%7D%7D%20%5Capprox%20%5Cfrac%7Bb%20-%20a%7D%7B3%7D%5Cleft%28%20%5Cfrac%7B3%7D%7B2%7D%5Cleft%28%20f_%7B1%7D%20%2B%20f_%7B2%7D%20%5Cright%29%20%5Cright%29."><br></p></center>
<p>One can use a computer to find the third coefficient. This gives us a Newton-Cotes formula known as the Milne rule:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7Ba%7D%5E%7Bb%7D%7Bf%5Cleft%28%20x%20%5Cright%29%5Ctext%7B%5C%20dx%7D%7D%20%5Capprox%20%5Cfrac%7Bb%20-%20a%7D%7B4%7D%5Cleft%28%20%5Cfrac%7B4%7D%7B3%7D%5Cleft%28%20%7B2f%7D_%7B1%7D%20-%20f_%7B2%7D%20%2B%20f_%7B3%7D%20%5Cright%29%20%5Cright%29."><br></p></center>
<p>This can be continued to get further Newton-Cotes formulae.</p>
<p>The idea of approximating an integral with a sum which is closely related is a common one in analysis. The Euler-Maclaurin formula, for example, gives the difference between the integral and the sum. In some cases, one can take a function defined by an integral and write it as a series expansion. As an example, start with the following function:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%5Cleft%28%20x%20%5Cright%29%20%3D%20%5Cint_%7B0%7D%5E%7B1%7D%5Cfrac%7B%5Csqrt%7Bt%7D%7D%7B%5Cleft%28%201%20-%20xt%5E%7B2%7D%20%5Cright%29%5E%7B%5Cbeta%7D%7D%5C%20dt%2C"><br></p></center>
<p>where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cbeta%20%3E%201">. It is known that</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cleft%28%201%20-%20z%20%5Cright%29%5E%7B-%20%5Cbeta%7D%20%3D%20%5Cfrac%7B1%7D%7B%5CGamma%5Cleft%28%20%5Cbeta%20%5Cright%29%7D%5Csum_%7Bk%20%3D%200%7D%5E%7B%5Cinfty%7D%5Cfrac%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29z%5E%7Bk%7D%7D%7Bk%21%7D%2C"><br></p></center>
<p>where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5CGamma"> is the gamma function. This function is essentially a generalisation of the factorial function to complex analysis. For some time, it was suggested that the Euler characteristic <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cchi%5Cleft%28%20%5Ctext%7BOut%7D%5Cleft%28%20F_%7Bn%7D%20%5Cright%29%20%5Cright%29"> might be related to number theory. Borinsky proved that this is the case using the gamma function. In particular:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Coverline%7B%5Cchi%7D%5Cleft%28%20%5Ctext%7BOut%7D%5Cleft%28%20F_%7Bn%20%2B%201%7D%20%5Cright%29%20%5Cright%29%20%3D%20-%20%5Cfrac%7B%5CGamma%5Cleft%28%20n%20%2B%20%5Cfrac%7B1%7D%7B2%7D%20%5Cright%29%7D%7Bn%5Clog%5E%7B2%7Dn%7D%20%2B%20O%5Cleft%28%20%5Cfrac%7B%5CGamma%5Cleft%28%20n%20%2B%20%5Cfrac%7B1%7D%7B2%7D%20%5Cright%29%7D%7Bn%5Clog%5E%7B4%7Dn%7D%20%5Cright%29."><br></p></center>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Coverline%7B%5Cchi%7D%5Cleft%28%20%5Ctext%7BOut%7D%5Cleft%28%20F_%7Bn%20%2B%201%7D%20%5Cright%29%20%5Cright%29"> grows at a faster than exponential rate, since the error term is not sufficient to cancel the growth. This solves a conjecture due to Vogtmann that one can take the Euler characteristic of the mapping class groups to get a relation with the Bernoulli numbers. Recall from the definition of the gamma function that</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B%5CGamma%5Cleft%28%20n%20%2B%20%5Cfrac%7B1%7D%7B2%7D%20%5Cright%29%20%3D%20%5Cfrac%7B2n%21%7D%7B4n%21%7D%5Csqrt%7B%5Cpi%7D%2C%0A%7D%7B%3D%20%5Cfrac%7B%5Cleft%28%202n%20-%201%20%5Cright%29%21%21%7D%7B2%5E%7Bn%7D%7D%5Csqrt%7B%5Cpi%7D.%7D"><br></p></center>
<p>This implies the nice identity:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5CGamma%5Cleft%28%20%5Cfrac%7B1%7D%7B2%7D%20%5Cright%29%20%3D%20%5Csqrt%7B%5Cpi%7D."><br></p></center>
<p>By comparison with the above series we have</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7Bf%5Cleft%28%20x%20%5Cright%29%20%3D%20%5Cint_%7B0%7D%5E%7B1%7D%7B%5Ctext%7Bdt%5C%20%7D%5Cfrac%7B%5Csqrt%7Bt%7D%7D%7B%5CGamma%5Cleft%28%20%5Cbeta%20%5Cright%29%7D%7D%5Csum_%7Bk%20%3D%200%7D%5E%7B%5Cinfty%7D%5Cfrac%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%7D%7Bk%21%7Dx%5E%7Bk%7Dt%5E%7B2k%7D%2C%0A%7D%7B%3D%20%5Cfrac%7B1%7D%7B%5CGamma%5Cleft%28%20%5Cbeta%20%5Cright%29%7D%5Csum_%7Bk%20%3D%200%7D%5E%7B%5Cinfty%7D%5Cfrac%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%7D%7Bk%21%7Dx%5E%7Bk%7D%5Cint_%7B0%7D%5E%7B1%7D%7B%5Ctext%7Bdt%5C%20%7Dt%5E%7B2k%20%2B%20%5Cfrac%7B1%7D%7B2%7D%7D%7D%2C%0A%7D%7B%3D%20%5Cfrac%7B1%7D%7B%5CGamma%5Cleft%28%20%5Cbeta%20%5Cright%29%7D%5Csum_%7Bk%20%3D%200%7D%5E%7B%5Cinfty%7D%5Cfrac%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29x%5E%7Bk%7D%7D%7Bk%21%5Cleft%28%202k%20%2B%20%5Cfrac%7B3%7D%7B2%7D%20%5Cright%29%7D%2C%0A%7D%7B%3D%20%5Cfrac%7B2%7D%7B%5CGamma%5Cleft%28%20%5Cbeta%20%5Cright%29%7D%5Csum_%7Bk%20%3D%200%7D%5E%7B%5Cinfty%7D%5Cfrac%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%7D%7Bk%21%5Cleft%28%204k%20%2B%203%20%5Cright%29%7Dx%5E%7Bk%7D.%7D"><br></p></center>
<p>We would now like to find the radius of convergence for the series. This can be done very easily by finding the limit of the ratio of coefficients:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?u_%7Bk%7D%20%3D%20%5Cfrac%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%7D%7Bk%21%5Cleft%28%204k%20%2B%203%20%5Cright%29%7D."><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B%5Cfrac%7Bu_%7Bk%7D%7D%7Bu_%7Bk%20%2B%201%7D%7D%20%3D%20%5Cfrac%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%7D%7Bk%21%5Cleft%28%204k%20%2B%203%20%5Cright%29%7D%5Cfrac%7B%5Cleft%28%20k%20%2B%201%20%5Cright%29%21%5Cleft%28%204k%20%2B%204%20%5Cright%29%7D%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%2B%201%20%5Cright%29%7D%2C%0A%7D%7B%3D%20%5Cfrac%7B%5Cleft%28%20k%20%2B%201%20%5Cright%29%5Cleft%28%204k%20%2B%204%20%5Cright%29%7D%7B4k%20%2B%203%7D%5Cfrac%7B%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%7D%7B%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%5CGamma%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%7D%2C%0A%7D%7B%3D%20%5Cfrac%7B%5Cleft%28%20k%20%2B%201%20%5Cright%29%5Cleft%28%204k%20%2B%204%20%5Cright%29%7D%7B%5Cleft%28%20k%20%2B%20%5Cbeta%20%5Cright%29%5Cleft%28%204k%20%2B%203%20%5Cright%29%7D.%7D"><br></p></center>
<p>This ratio tends to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1"> as <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?k"> tends to infinity, so the radius of convergence is <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1">. We now take the ratio of the coefficient with the previous coefficient and do some algebra which we will show.</p>

<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B%5Cfrac%7Bu_%7Bk%7D%7D%7Bu_%7Bk%20-%201%7D%7D%20=%20%5Cfrac%7B%5CGamma%5Cleft(%20k%20-%20%5Cbeta%20%5Cright)%7D%7Bk!%5Cleft(%202k%20+%20%5Cfrac%7B1%7D%7B2%7D%20+%201%20%5Cright)%7D%5Cfrac%7B%5Cleft(%20k%20-%201%20%5Cright)!%5Cleft(%202k%20+%20%5Cfrac%7B1%7D%7B2%7D%20-%202%20%5Cright)%7D%7B%5CGamma%5Cleft(%20k%20+%20%5Cbeta%20-%201%20%5Cright)%7D,%0A%7D"><br></p></center>

<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B=%20%5Cfrac%7B%5Cleft(%20k%20+%20%5Cbeta%20-%201%20%5Cright)%5Cleft(%202k%20+%20%5Cfrac%7B1%7D%7B2%7D%20-%202%20%5Cright)%7D%7Bk%5Cleft(%202k%20+%20%5Cfrac%7B1%7D%7B2%7D%20+%201%20%5Cright)%7D,%0A%7D"><br></p></center>

<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B=%20%5Cleft(%201%20+%20%5Cfrac%7B%5Cbeta%5E%7B-%201%7D%7D%7Bk%7D%20%5Cright)%5Cleft(%201%20-%20%5Cfrac%7B3%7D%7B4k%7D%20%5Cright)%5Cleft(%201%20+%20%5Cfrac%7B3%7D%7B4k%7D%20%5Cright)%5E%7B-%201%7D,%0A%7D"><br></p></center>

<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B=%20%5Cleft(%201%20-%20%5Cfrac%7B3%7D%7B4k%7D%20+%20%5Cfrac%7B%5Cbeta%5E%7B-%201%7D%7D%7Bk%7D%20+%20O%5Cleft(%20k%5E%7B-%202%7D%20%5Cright)%20%5Cright)%5Cleft(%201%20-%20%5Cfrac%7B3%7D%7B4k%7D%20+%20O%5Cleft(%20k%5E%7B-%202%7D%20%5Cright)%20%5Cright),%0A%7D"><br></p></center>

<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B=%201%20-%20%5Cfrac%7B3%7D%7B4k%7D%20+%20%5Cfrac%7B%5Cbeta%5E%7B-%201%7D%7D%7Bk%7D%20-%20%5Cfrac%7B3%7D%7B4k%7D%20+%20O%5Cleft(%20k%5E%7B-%202%7D%20%5Cright),%0A%7D"><br></p></center>

<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B=%201%20+%20%5Cfrac%7B4%5Cleft(%20%5Cbeta%20-%201%20%5Cright)%20-%206%7D%7B4k%7D%20+%20O%5Cleft(%20k%5E%7B-%202%7D%20%5Cright),%0A%7D"><br></p></center>

<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B=%201%20+%20%5Cfrac%7B4%5Cbeta%20-%2010%7D%7B4k%7D%20+%20O%5Cleft(%20k%5E%7B-%202%7D%20%5Cright),%0A%7D"><br></p></center>

<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%7B=%201%20-%20%5Cfrac%7B%5Cfrac%7B10%7D%7B4%7D%20-%20%5Cbeta%7D%7Bk%7D%20+%20O%5Cleft(%20k%5E%7B-%202%7D%20%5Cright).%7D"><br></p></center>

<p>Comparing with a result of Richards, we see that there is a singularity of order <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Calpha%20%3D%201%20-%20%5Cbeta">, since<a href="#fn239" class="footnote-ref" id="fnref239" role="doc-noteref"><sup>239</sup></a></p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%5Cleft%28%20x%20%5Cright%29%5C%20%5Csim%5C%20A%5Cleft%28%201%20-%20x%20%5Cright%29%5E%7B1%20-%20%5Cbeta%7D%2C"><br></p></center>
<p>for some constant <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A"> and for <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x"> close to the radius of convergence.</p>
<p>Another common method of numerical integration is Gaussian quadrature. One approximates a definite integral representing a function as a weighted sum over <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?N"> points:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B-%201%7D%5E%7B1%7D%7Bw%5Cleft%28%20x%20%5Cright%29f%5Cleft%28%20x%20%5Cright%29%5Ctext%7B%5C%20dx%7D%7D%20%5Capprox%20%5Csum_%7Bk%20%3D%200%7D%5E%7BN%7D%7Bc_%7Bk%7Df%5Cleft%28%20x_%7Bk%7D%20%5Cright%29%7D%2C"><br></p></center>
<p>where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?w%5Cleft%28%20x%20%5Cright%29"> is a weight function. The weight function is often the unit function, but not always. Be warned that basic quadrature commands in computer algebra programs will likely not look for singularities, as they will probably assume a unit weight function. As we know, the inner product notation denotes</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cleft%28%20f%2Cg%20%5Cright%29%20%3D%20%5Cint_%7Ba%7D%5E%7Bb%7D%7Bw%5Cleft%28%20x%20%5Cright%29f%5Cleft%28%20x%20%5Cright%29g%5Cleft%28%20x%20%5Cright%29%5Ctext%7B%5C%20dx%7D%7D."><br></p></center>
<p>If we take the functions in this integral to be the set of orthogonal polynomials <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bk%7D%5Cleft%28%20x%20%5Cright%29"> such that the leading term of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cphi_%7Bn%7D"> is <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x%5E%7Bn%7D">, we can assume that they satisfy an orthogonality condition</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cleft%28%20%5Cphi_%7Bn%7D%2C%5Cphi_%7Bm%7D%20%5Cright%29%20%3D%20h_%7Bn%7D%5Cdelta_%7B%5Ctext%7Bnm%7D%7D."><br></p></center>
<p>Written out explicitly, we might choose the weight function so that we have</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B0%7D%5E%7B1%7D%7B%5Cleft%28%201%20-%20x%20%5Cright%29%5E%7B3%2F2%7D%5C%20%5Cphi_%7Bn%7D%5Cleft%28%20x%20%5Cright%29%5Cphi_%7Bm%7D%5Cleft%28%20x%20%5Cright%29%5C%20dx%20%3D%20h_%7Bn%7D%5Cdelta_%7B%5Ctext%7Bnm%7D%7D%7D."><br></p></center>
<p>These polynomials can be found using Gram-Schmidt orthogonalisation. For example, the first few polynomials would be</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cphi_%7B0%7D%20%3D%201%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cphi_%7B1%7D%20%3D%20x%20-%20%5Cfrac%7B2%7D%7B7%7D%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cphi_%7B2%7D%20%3D%20x%5E%7B2%7D%20-%20%5Cfrac%7B8%7D%7B11%7Dx%20%2B%20%5Cfrac%7B8%7D%7B99%7D%2C"><br></p></center>
<p>and so on. The corresponding coefficients of the Kronecker delta are seen to be:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?h_%7B0%7D%20%3D%20%5Cfrac%7B2%7D%7B5%7D%2C%5C%20h_%7B1%7D%20%3D%20%5Cfrac%7B8%7D%7B441%7D%2C%5C%20h_%7B2%7D%20%3D%20%5Cfrac%7B128%7D%7B127%2C413%7D."><br></p></center>
<p>Now let us say that we wish to approximate the following definite integral:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B0%7D%5E%7B1%7D%5Cleft%28%201%20-%20x%20%5Cright%29%5E%7B3%2F2%7Df%5Cleft%28%20x%20%5Cright%29%5Ctext%7B%5C%20dx.%7D"><br></p></center>
<p>Write</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%5Cleft%28%20x%20%5Cright%29%20%3D%20%5Csum_%7Bj%20%3D%200%7D%5E%7B%5Cinfty%7D%7Ba_%7Bj%7D%5Cphi_%7Bj%7D%5Cleft%28%20x%20%5Cright%29%7D%2C"><br></p></center>
<p>where</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?a_%7Bj%7D%20%3D%20%5Cfrac%7B1%7D%7Bh_%7Bj%7D%7D%5Cint_%7B0%7D%5E%7B1%7D%7Bw%5Cleft%28%20x%20%5Cright%29%7D%5Cphi_%7Bj%7D%5Cleft%28%20x%20%5Cright%29f%5Cleft%28%20x%20%5Cright%29%5Ctext%7B%5C%20dx.%7D"><br></p></center>
<p>You may recognise the similarity to Fourier coefficients. Choose <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x_%7Bk%7D"> to be the <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?N"> zeroes of the polynomial <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cphi_%7BN%7D%5Cleft%28%20x%20%5Cright%29"> and the <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?c_%7Bk%7D"> to solve the linear equations</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Csum_%7Bj%20%3D%201%7D%5E%7BN%7D%7Bc_%7Bj%7D%20%3D%20h_%7B0%7D%7D%2C%5C%20%5Csum_%7Bj%20%3D%201%7D%5E%7BN%7Dc_%7Bj%7Dp_%7Bk%7D%5Cleft%28%20x_%7Bj%7D%20%5Cright%29%2C"><br></p></center>
<p>for <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?k"> up to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?N%20-%201">.</p>
<p>Let us choose <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?N%20%3D%204">, then we have an approximation</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B0%7D%5E%7B1%7D%5Cleft%28%201%20-%20x%20%5Cright%29%5E%7B3%2F2%7Df%5Cleft%28%20x%20%5Cright%29%5C%20dx%20%5Capprox%20%5Csum_%7Bk%20%3D%201%7D%5E%7B4%7D%7Bc_%7Bk%7Df%5Cleft%28%20x_%7Bk%7D%20%5Cright%29%7D%2C"><br></p></center>
<p>where the numbers <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x_%7Bk%7D"> are the zeroes of the fourth-order orthogonal polynomial <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cphi_%7B4%7D"> and the coefficients are given by a tuple as follows</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?c%20%3D%20%5Cleft%5Clbrack%200.122%2C%5C%200.169%2C%5C%200.092%2C%5C%200.017%20%5Cright%5Crbrack."><br></p></center>
<p>This quadrature can be implemented explicitly with a computer to get numerical values for certain definite integrals. For example,</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B0%7D%5E%7B1%7D%5Cleft%28%201%20-%20x%20%5Cright%29%5E%7B3%2F2%7D%5Ccos%5Cleft%28%20%5Cpi%5Csin%20x%20%5Cright%29%5C%20dx%20%5Capprox%200.221."><br></p></center>
<p>The Gauss quadrature method can be extended to allow for approximations of integrals of the form</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B-%201%7D%5E%7B1%7D%5Cfrac%7Bf%5Cleft%28%20x%20%5Cright%29%7D%7B%5Csqrt%7B1%20-%20x%5E%7B2%7D%7D%7D%5Ctext%7B%5C%20dx.%7D"><br></p></center>
<p>In this case, the quadrature is Chebyshev-Gauss quadrature. If we take the weight function to be <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?e%5E%7B-%20x%5E%7B2%7D%7D">, we have Gauss-Hermite quadrature for evaluating integrals of the form</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B-%20%5Cinfty%7D%5E%7B%5Cinfty%7D%7Be%5E%7B-%20x%5E%7B2%7D%7Df%5Cleft%28%20x%20%5Cright%29%7D%5Ctext%7B%5C%20dx.%7D"><br></p></center>
<p>Note that this allows for integration over an infinite interval.</p>
<p>Holtz has worked in particular on matrix orthogonal polynomials, extending some classical results from the theory of orthogonal polynomials on the unit circle to the matrix case. Holtz proved what can be called a matrix Szegő theorem: for any matrix probability measure <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Csigma%20%5Cin%20P_%7B%5Cmathcal%7Bl%7D%7D%5Cleft%28%20%5Cmathbb%7BT%7D%20%5Cright%29"> and any natural number <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?n">, one has</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B%5Cmathbb%7BT%7D%7D%5E%7B%5C%20%7D%7B%5Ctext%7BTr%7D%5Clog%7B%5Csigma%27%7D%7D%5Cfrac%7B%5Ctext%7Bd%CE%B8%7D%7D%7B2%5Cpi%7D%20%5Cleq%20%5Ctext%7BTr%7D%5Clog%5Cbeta_%7Bn%7D%20%3D%20%5Clog%7B%5Cprod_%7Bk%20%3D%200%7D%5E%7Bn%20-%201%7D%7B%5Cdet%7B%281%20-%20%5Calpha_%7Bk%7D%5E%7B%2A%7D%7D%7D%7D%5Calpha_%7Bk%7D%29."><br></p></center>
<p>As a corollary, one has that if <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Csigma"> is a Szegő measure, then</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cint_%7B%5Cmathbb%7BT%7D%7D%5E%7B%5C%20%7D%7B%5Ctext%7BTr%7D%5Clog%7B%5Csigma%27%7D%7D%5Cfrac%7B%5Ctext%7Bd%CE%B8%7D%7D%7B2%5Cpi%7D%20%5Cleq%20%5Cinf_%7Bn%7D%7B%5Ctext%7BTr%7D%5Clog%5Cbeta_%7Bn%7D%7D%20%5Cleq%20-%20%5Csup_%7Bn%7D%5Clog%5Cleft%5C%7C%20%5Cbeta_%7Bn%7D%5E%7B-%201%7D%20%5Cright%5C%7C%20%5Cleq%200."><br></p></center>
<p>Using these techniques, she obtained an elementary proof of the Helson-Lowdenslager distance formula, which states that for every <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Csigma%20%5Cin%20P_%7B%5Cmathcal%7Bl%7D%7D%5Cleft%28%20%5Cmathbb%7BT%7D%20%5Cright%29">,</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cexp%5Cint_%7B%5Cmathbb%7BT%7D%7D%5E%7B%5C%20%7D%7B%5Cfrac%7B1%7D%7B%5Cmathcal%7Bl%7D%7D%5Ctext%7BTr%7D%5Clog%7B%5Csigma%27%7D%7D%5Cfrac%7B%5Ctext%7Bd%CE%B8%7D%7D%7B2%5Cpi%7D%20%3D%20%5Cinf_%7BA%2CP%7D%5Cint_%7B%5Cmathbb%7BT%7D%7D%5E%7B%5C%20%7D%5Cfrac%7B1%7D%7B%5Cmathcal%7Bl%7D%7D%5Ctext%7BTr%7D%5Cleft%28%20%5Cleft%28%20A%20%2B%20P%20%5Cright%29%5E%7B%2A%7D%5Ctext%7Bd%CF%83%7D%5Cleft%28%20A%20%2B%20P%20%5Cright%29%20%5Cright%29%2C"><br></p></center>
<p>where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A"> cycles through all matrices of unit determinant, and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?P"> runs over all trigonometric polynomials of the form<a href="#fn240" class="footnote-ref" id="fnref240" role="doc-noteref"><sup>240</sup></a></p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?P%5Cleft%28%20e%5E%7B%5Ctext%7Bi%CE%B8%7D%7D%20%5Cright%29%20%3D%20%5Csum_%7Bk%20%3E%200%7D%5E%7B%5C%20%7D%7BA_%7Bk%7De%5E%7B%5Ctext%7Bik%CE%B8%7D%7D%7D."><br></p></center>
<p>Most of us are familiar with Gram-Schmidt orthogonalisation. More advanced numerical linear algebra considers iterative methods which are suitable for large sparse systems. We mentioned earlier the Gauss-Seidel and conjugate gradient methods, and stated that the Gauss-Seidel method is not as efficient as the conjugate gradient method. In fact, this can be quantified by including a comparison between both methods in terms of the computational time needed to solve a randomly generated positive definite <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20n"> matrix with density <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.1">. The comparison is conjugate gradient against Gauss-Seidel with two different preconditioners<a href="#fn241" class="footnote-ref" id="fnref241" role="doc-noteref"><sup>241</sup></a>.</p>
<table>
<thead>
<tr class="header">
<th><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?n"><br></th>
<th>Gauss-Seidel</th>
<th>Gauss-Seidel (RCM)</th>
<th>Gauss-Seidel (MD)</th>
<th>CG</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?100"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.007928804"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.007056294"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.006839876"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.002512009"><br></td>
</tr>
<tr class="even">
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?500"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.502849338"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.405763259"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.303914523"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.002419189"><br></td>
</tr>
<tr class="odd">
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1000"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?3.219242909"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?3.281714839"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?2.618037809"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.004361086"><br></td>
</tr>
<tr class="even">
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?3000"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?116.8587276"><br></td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?105.7410554"><br></td>
<td>99.96394861</td>
<td><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?0.044996817"><br></td>
</tr>
</tbody>
</table>

<p>One can see that the conjugate gradient method remains much more efficient even for very large matrices. Technically, conjugate gradient is an example of a Krylov space method. A crucial part of both methods is the minimization of some measure of error at the <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?k">th iteration. Another such method is GMRES (generalized minimal residual method). GMRES can be used for non-symmetric systems but one must store a basis for the <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?k">th Krylov space <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?K_%7Bk%7D"> and the algorithm must be re-started at some point, which increases the convergence time. The <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?k">th iteration of GMRES is the solution to a least squares problem:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctext%7Bminimize%7D_%7Bx%20%5Cin%20x_%7B0%7D%20%2B%20K_%7Bk%7D%7D%5Cleft%5C%7C%20b%20-%20Ax%20%5Cright%5C%7C_%7B2%7D."><br></p></center>
<p>Iterative methods are generally effective provided that the problem is preconditioned (where the preconditioner transforms the problem to one more amenable to iterative methods). In practice, preconditioning often involves an attempt to reduce the condition number, where the condition number measures the sensitivity of a function to changes in the input. Reducing the condition number improves the performance of the iteration, as does transforming the problem so that the eigenvalues are clustered near <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1">.</p>
<p>As an example, when performing an iteration we may wish to replace the linear system</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?Ax%20%3D%20b"><br></p></center>
<p>in the conjugate gradient method with another system which is symmetric positive definite and which has the same solution. We could do this by expressing the preconditioned problem in terms of a new symmetric positive definite matrix <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?B"> such that</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A%20%3D%20B%5E%7B2%7D"><br></p></center>
<p>and then use a two-sided preconditioner which approximates <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?B%5E%7B-%201%7D">. The square of this preconditioner is equal to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M">, where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M"> is a symmetric positive definite matrix close to <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A%5E%7B-%201%7D"> and so we can now express the preconditioned system in terms of the matrix <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Ctext%7BSAS%7D"> which has its eigenvalues clustered near <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1">.</p>
<p>It is also possible to study large dense, as opposed to large sparse, systems. The methods for solving these systems are obviously different, since the main operation in the sparse case is that of matrix-vector multiplication, which is only going to have linear computational cost for a sparse system. One way of dealing with a dense system is to replace the original matrix <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A"> with an approximate matrix <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cwidetilde%7BA%7D"> such that the computational cost of a matrix-vector product is much less than <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?n%5E%7B2%7D">, ie. the computational cost is reasonable. To be more specific, the computational cost of a matrix-vector multiplication with the new approximate matrix should be should be bounded by <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?O%5Cleft%28%20%5Ctext%7Bn%5C%20%7D%5Clog%5E%7B%5Calpha%7Dn%20%5Cright%29">, the memory needed to store the matrix and the time required to generate the matrix should both be bounded by <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?O%5Cleft%28%20n%5Ctext%7B%5C%20log%7D%5E%7B%5Cbeta%7Dn%20%5Cright%29">, and for some positive <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> and a matrix norm <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7C%20%5Cbullet%20%5Cright%5C%7C">, the approximate matrix should satisfy</p>
<p><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cleft%5C%7C%20A%20-%20%5Cwidetilde%7BA%7D%20%5Cright%5C%7C%20%5Cleq%20%5Cvarepsilon."></p>
<p>In both cases, <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cbeta"> should not depend on <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?n">.</p>
<p>You might wonder what kind of problem would produce dense matrices. When we are interpolating with radial basis functions, we form an interpolant of the form</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?s%5Cleft%28%20%5Cmathbf%7Bx%7D%20%5Cright%29%20%3D%20%5Csum_%7Bj%20%3D%201%7D%5E%7Bn%7D%7B%5Calpha_%7Bj%7DK%5Cleft%28%20%5Cmathbf%7Bx%7D%2C%5Cmathbf%7Bx%7D_%7Bj%7D%20%5Cright%29%7D%2C"><br></p></center>
<p>where the kernel is positive definite</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?K%5Cleft%28%20%5Cmathbf%7Bx%7D%2C%5Cmathbf%7By%7D%20%5Cright%29%20%3D%20%5CPhi%5Cleft%28%20%5Cmathbf%7Bx%7D%20-%20%5Cmathbf%7By%7D%20%5Cright%29%20%3D%20%5Cphi%5Cleft%28%20%5Cleft%5C%7C%20%5Cmathbf%7Bx%7D%20-%20%5Cmathbf%7By%7D%20%5Cright%5C%7C_%7B2%7D%20%5Cright%29."><br></p></center>
<p>The interpolation conditions create a system</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A%5Calpha%20%3D%20f"><br></p></center>
<p>with an interpolation matrix <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A"> which is symmetric positive definite. This matrix will be positive when the kernel does not have local support. In this case, it so happens that the computational cost of computing the matrix entries is cheap despite the matrix being dense, but this is not a typical example<a href="#fn242" class="footnote-ref" id="fnref242" role="doc-noteref"><sup>242</sup></a>.</p>
<p>Another classic of numerical linear algebra which we could mention is Gaussian elimination. This method was known to the Chinese linear algebraists, and rediscovered by Gauss. The result was also known to Newton (as was any method of elementary algebra or linear algebra). Gaussian elimination provides an algorithm for solving systems of simultaneous linear equations. The general procedure is as follows: take a system of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?n"> linear equations with a coefficient matrix <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A">. Write down the augmented matrix <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A%7Cb"> with <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?n"> rows <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?R_%7Bn%7D">. Subtract multiples of the first row from the other rows until the elements below the leading diagonal are zero in the first column. Repeat the process with multiples of the second row until the elements below the leading diagonal are zero in the second column. Continue doing this until we have an augmented matrix such that the coefficient matrix is upper triangular. Solve the corresponding system of equations by plugging everything in backwards. We will show how this might look in an explicit example. Start with a system of linear equations:</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x%20-%202y%20%2B%205z%20%3D%206%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x%20%2B%203y%20-%204z%20%3D%207%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?2%20%2B%206y%20-%2012z%20%3D%2012."><br></p></center>
<p>We need the elements below the leading diagonal in the first column to vanish. This can be done by performing elementary operations with the first row: for example, subtracting the first row from the second row would be the obvious thing to do. For the third row, you have to multiply the first row by <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?2"> before you subtract it. We then have</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x%20-%202y%20%2B%205z%20%3D%206%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?5y%20-%209z%20%3D%201%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?10y%20-%2022z%20%3D%200."><br></p></center>
<p>I will not bother actually writing everything out so that the coefficients are entries in an augmented matrix, but you can do this if you find it useful. Repeat the process by performing elementary operations with the second row until the element below the leading diagonal in the second column vanishes.</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x%20-%202y%20%2B%205z%20%3D%206%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?5y%20-%209z%20%3D%201%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?-%204z%20%3D%20-%202."><br></p></center>
<p>The rows of <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A"> are linearly independent so there is a unique solution to this system, which can be found trivially. This method can break down in various ways and will not always work but I will not go into this, as it can be found in any book on linear algebra. An application of the method is in polynomial interpolation.</p>
<p>In the field of linear algebra, Holtz has studied the conditions under which a matrix of differentiable functions has to commute with its element-wise derivative. She posed the problem of whether a matrix over a differential field <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D"> with an algebraically closed field of constants such that</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?MM%5E%7B%27%7D%20%3D%20M%27M"><br></p></center>
<p>has to have eigenvalues which are elements of the field <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D">. A differential field is an algebraic field together with an additional operation called the derivative which satisfies</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cleft%28%20a%20%2B%20b%20%5Cright%29%5E%7B%27%7D%20%3D%20a%5E%7B%27%7D%20%2B%20b%5E%7B%27%7D%2C"><br></p></center>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cleft%28%20%5Ctext%7Bab%7D%20%5Cright%29%5E%7B%27%7D%20%3D%20ab%5E%7B%27%7D%20%2B%20a%5E%7B%27%7D%5Ctext%7Bb.%7D"><br></p></center>
<p>She also proves various other results relating to triangularizability and diagonalizability of matrices in this context: for example, if <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D"> is a differential field with an algebraically closed field of constants <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BK%7D"> and if <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M%20%5Cin%20%5Cmathbb%7BF%7D%5E%7Bn%2Cn%7D"> is type <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1">, then <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M"> is <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BK%7D">-triangularizable.</p>
<p>This theorem implies that type <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1"> matrices have <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?n"> eigenvalues in <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BF%7D"> if <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BK%7D"> is algebraically closed. A type <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?1"> matrix is one which has the form</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M_%7B1%7D%20%3D%20%5Csum_%7B%5Clambda%7D%5E%7B%5C%20%7D%7Bf_%7B%5Clambda%7DC_%7B%5Clambda%7D%7D%2C"><br></p></center>
<p>where the <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?C_%7B%5Clambda%7D"> are pairwise commuting constant matrices. A type 2 matrix is one which has the form</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?M_%7B2%7D%20%3D%20%5Cleft%28%20f_%7B%5Calpha%7Dg_%7B%5Cbeta%7D%20%5Cright%29%2C"><br></p></center>
<p>where <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_%7B1%7D%2C%5C%20%5Cldots%2C%5C%20f_%7Bn%7D%2C%5C%20g_%7B1%7D%2C%5C%20%5Cldots%2C%5C%20g_%7Bn%7D"> are arbitrary functions satisfying the conditions</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Csum_%7B%5Calpha%7D%5E%7B%5C%20%7D%7Bf_%7B%5Calpha%7Dg_%7B%5Calpha%7D%7D%20%3D%20%5Csum_%7B%5Calpha%7D%5E%7B%5C%20%7D%7B%7Bf_%7B%5C%20%7D%7D_%7B%5Calpha%7D%5E%7B%27%7Dg_%7B%5Calpha%7D%7D%20%3D%200%2C"><br></p></center>
<p>which implies that</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Csum_%7B%5Calpha%7D%5E%7B%5C%20%7D%7B%7Bf_%7B%5C%20%7D%7D_%7B%5Calpha%7D%5E%7B%5C%20%7D%7Bg_%7B%5C%20%7D%7D_%7B%5Calpha%7D%5E%7B%27%7D%7D%20%3D%200."><br></p></center>
<p>This distinction can be found in a letter of Schur<a href="#fn243" class="footnote-ref" id="fnref243" role="doc-noteref"><sup>243</sup></a>.</p>
<p>Closely allied to numerical analysis is a huge body of theory called approximation theory. Consider a curve in the plane which is given by a function and then think about how one might draw a straight line which best fits the curve. This is the prototypical problem in approximation theory. One has a function or a member of a set which needs to be approximated, a set of possible approximations <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> (the set of all straight lines in the example we have given), and some method for selecting the best approximation from the set <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D">. How can one measure how good an approximation is? Recall that metric spaces are equipped with a distance function which could be used to do this, and it is often the case when working with an approximation problem that there is a nice metric space which contains both the function to be approximated and the set of approximations <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D">. We could say that <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?a_%7B1%7D"> is a better approximation than <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?a_%7B2%7D"> if the inequality</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?d%5Cleft%28%20a_%7B1%7D%2Cf%20%5Cright%29%20%3C%20d%5Cleft%28%20a_%7B2%7D%2Cf%20%5Cright%29"><br></p></center>
<p>is satisfied. We can define <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?a_%7BB%7D"> to be the best approximation if the inequality</p>
<center><p><br><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?d%5Cleft%28%20a_%7BB%7D%2Cf%20%5Cright%29%20%3C%20d%5Cleft%28%20a%2Cf%20%5Cright%29"><br></p></center>
<p>holds for all <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?a%20%5Cin%20%5Cmathcal%7BA%7D">. Bear in mind that the metric space needs to be able to measure the error when we attempt a trial approximation so that we can improve our attempts to get a best fit. Some famous results which you might be familiar with (the Stone-Weierstrass theorem and the Lagrange interpolation formula, for example) are results of approximation theory<a href="#fn244" class="footnote-ref" id="fnref244" role="doc-noteref"><sup>244</sup></a>.</p>
